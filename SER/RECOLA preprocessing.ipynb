{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import json\n",
    "import soundfile as sf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "root_dir = '/home/mabikana/Documents/PhD/Datasets/RECOLA/'\n",
    "emotional_data_dir = root_dir + 'RECOLA-Annotation/emotional_behaviour/'\n",
    "valence_data_dir = emotional_data_dir + '/valence/'\n",
    "arousal_data_dir = emotional_data_dir + '/arousal/'\n",
    "audio_data_dir = root_dir + 'RECOLA-Audio-recordings/'\n",
    "chunks_dir = root_dir + 'audio_chunks/'\n",
    "resampled_chunks_dir = root_dir + 'audio_chunks_reduced_sr/'\n",
    "pickle_input_file = '/home/mabikana/Documents/PhD/SER Code/RECOLA Processing/melspectrograms.pickle'\n",
    "text_input_file = '/home/mabikana/Documents/PhD/SER Code/RECOLA Processing/text.pickle'\n",
    "tokenizer_file = '/home/mabikana/Documents/PhD/SER Code/RECOLA Processing/tokenizer_file.pickle'\n",
    "pickle_valence_file = '/home/mabikana/Documents/PhD/SER Code/RECOLA Processing/valence.pickle'\n",
    "pickle_arousal_file = '/home/mabikana/Documents/PhD/SER Code/RECOLA Processing/arousal.pickle'\n",
    "pickle_spectrogram_scaler_file = '/home/mabikana/Documents/PhD/SER Code/RECOLA Processing/melspectrograms_scaler.pickle'\n",
    "pickle_arousal_scaler_file = '/home/mabikana/Documents/PhD/SER Code/RECOLA Processing/arousal_scaler.pickle'\n",
    "pickle_valence_scaler_file = '/home/mabikana/Documents/PhD/SER Code/RECOLA Processing/valence_scaler.pickle'\n",
    "speech_to_text_model_dir = '/home/mabikana/Documents/PhD/SER Code/EmergencyOutcomePrediction/speech_to_text/model'\n",
    "speech_to_text_model = Model(speech_to_text_model_dir)\n",
    "\n",
    "speech_rate = 44100\n",
    "dur = 4\n",
    "dur_multiplier = 25\n",
    "target_speech_rate = 8000\n",
    "\n",
    "# Large vocabulary free form recognition\n",
    "rec = KaldiRecognizer(speech_to_text_model, target_speech_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 382813.46it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 140.06it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 149.23it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data = {}\n",
    "\n",
    "for audio_file in tqdm(os.listdir(audio_data_dir)):\n",
    "    all_data[audio_file.split(\".\")[0]] = {}\n",
    "\n",
    "\n",
    "for valence_file in tqdm(os.listdir(valence_data_dir)):\n",
    "    valence_data = {}\n",
    "    with open(valence_data_dir + valence_file) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=';')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                line_count += 1\n",
    "            else:\n",
    "                valence_data[row[0]] = {}\n",
    "                valence_data[row[0]][\"valence\"] = float(row[1]) # extract time and only first annotator's values\n",
    "                line_count += 1\n",
    "\n",
    "    all_data[valence_file.split(\".\")[0]] = valence_data\n",
    "\n",
    "\n",
    "\n",
    "for arousal_file in tqdm(os.listdir(arousal_data_dir)):\n",
    "    arousal_data = {}\n",
    "    with open(arousal_data_dir + arousal_file) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=';')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                line_count += 1\n",
    "            else:\n",
    "                all_data[arousal_file.split(\".\")[0]][row[0]][\"arousal\"] = float(row[1])\n",
    "                line_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = list(all_data.keys())[0]\n",
    "# print(\"Values for \" + file)\n",
    "# print(all_data[file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save spectrograms\n",
    "def get_log_mel_spectrogram(path, n_fft, hop_length, n_mels, file_name):\n",
    "    y, sr = librosa.load(path, sr=speech_rate, duration=dur)\n",
    "    signal = librosa.resample(y, orig_sr=speech_rate, target_sr=target_speech_rate)\n",
    "    resampled_chunk_name = resampled_chunks_dir + file_name\n",
    "    sf.write(resampled_chunk_name, signal, target_speech_rate)\n",
    "        \n",
    "    audio_len = speech_rate * dur\n",
    "    \n",
    "    file_length = np.size(y)\n",
    "    if file_length != audio_len:\n",
    "        y = np.concatenate((y, np.zeros(audio_len-file_length)), axis=0)\n",
    "    \n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    log_mel_spectrogram = log_mel_spectrogram.reshape(-1,1)\n",
    "    \n",
    "\n",
    "    return log_mel_spectrogram, resampled_chunk_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_to_text(chunk_path):\n",
    "    textResults = []\n",
    "    wf = open(chunk_path, \"rb\")\n",
    "    wf.read(44) # skip header\n",
    "\n",
    "    while True:\n",
    "        data = wf.read(2000)\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        if rec.AcceptWaveform(data):\n",
    "            res = json.loads(rec.Result())\n",
    "    \n",
    "    resultDict = json.loads(rec.FinalResult())\n",
    "    textResults.append(resultDict.get(\"text\", \"\"))\n",
    "    \n",
    "    return ' '.join(textResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [1:01:46<00:00, 161.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max text length = 115\n",
      "Min text length = 0\n",
      "Avg text length = 19.175663845929385\n",
      "37.669827\n",
      "1.0000001\n",
      "0.9\n",
      "0.9999999999999999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3427, 128, 345, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_spectrograms = []\n",
    "num_rows = dur * dur_multiplier\n",
    "valence = []\n",
    "arousal = []\n",
    "wav_text_data = []\n",
    "max_text_len = 0\n",
    "min_text_len = 999999\n",
    "all_lens = []\n",
    "\n",
    "for file in tqdm(os.listdir(audio_data_dir)):\n",
    "    chunk_length_ms = dur * 1000\n",
    "    frequency, signal = wavfile.read(audio_data_dir + '/' + file)\n",
    "\n",
    "    slice_length = int(chunk_length_ms / 1000) # in seconds\n",
    "    overlap =  int((chunk_length_ms / 1000) / 2) # in seconds\n",
    "    slices = np.arange(0, len(signal)/frequency, slice_length-overlap, dtype=int)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    valence_arousal_values = all_data[file.split(\".\")[0]]\n",
    "    chunk_counter = 0\n",
    "\n",
    "    for start, end in zip(slices[:-1], slices[1:]):\n",
    "        start_audio = start * frequency\n",
    "        end_audio = (end + overlap)* frequency\n",
    "        audio_slice = signal[int(start_audio): int(end_audio)]\n",
    "        chunks.append(audio_slice)\n",
    "        chunk_name = chunks_dir + '/' + file + '_{0}.wav'.format(i)\n",
    "        wavfile.write(chunk_name, speech_rate, audio_slice)\n",
    "        chunk_spectrogram, resampled_chunk_name = get_log_mel_spectrogram(path=chunk_name, \\\n",
    "                                                                          n_fft=2048, hop_length=512, n_mels=128,\\\n",
    "                                                                          file_name=file + '_{0}.wav'.format(i))\n",
    "        all_spectrograms.append(chunk_spectrogram)\n",
    "        chunks_row_start = int(chunk_counter)\n",
    "        chunks_row_end = int(num_rows + chunk_counter)\n",
    "        chunk_valence_arousal_values = list(valence_arousal_values.values())[chunks_row_end]\n",
    "        chunk_valence_values = chunk_valence_arousal_values[\"valence\"]\n",
    "        chunk_arousal_values = chunk_valence_arousal_values[\"arousal\"]\n",
    "        valence.append(chunk_valence_values)\n",
    "        arousal.append(chunk_arousal_values)\n",
    "#         print(\"For file \" + file + \" and chunk start \", chunks_row_start, \" and end \", chunks_row_end)\n",
    "#         print(\"valence \", chunk_valence_values ,\" and arousal \", chunk_arousal_values, \"\\n\")\n",
    "        chunk_counter += num_rows / overlap\n",
    "\n",
    "\n",
    "        # Extract text\n",
    "        text = chunk_to_text(resampled_chunk_name)\n",
    "\n",
    "        if(len(text) > max_text_len):\n",
    "            max_text_len = len(text)\n",
    "        if(len(text) < min_text_len):\n",
    "            min_text_len = len(text)\n",
    "            \n",
    "#         print(\"\\nFor chunk \", chunk_name, \" text is: \", text)\n",
    "        all_lens.append(len(text))\n",
    "        wav_text_data.append(text)\n",
    "    \n",
    "        i += 1\n",
    "\n",
    "\n",
    "print('Max text length = ' + str(max_text_len))\n",
    "print('Min text length = ' + str(min_text_len))\n",
    "print('Avg text length = ' + str(np.mean(all_lens)))\n",
    "\n",
    "# normalize spectrograms\n",
    "all_spectrograms = np.array(all_spectrograms) \n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "arousal_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "valence_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "all_spectrograms = all_spectrograms.reshape(len(all_spectrograms),-1)\n",
    "scaler.fit(all_spectrograms)\n",
    "valence = np.array(valence)\n",
    "arousal = np.array(arousal)\n",
    "\n",
    "valence = valence.reshape(len(valence),-1)\n",
    "arousal = arousal.reshape(len(arousal),-1)\n",
    "\n",
    "arousal_scaler.fit(arousal)\n",
    "valence_scaler.fit(valence)\n",
    "\n",
    "normalized_melspectrograms = scaler.transform(all_spectrograms)\n",
    "normalized_valence = valence_scaler.transform(valence)\n",
    "normalized_arousal = arousal_scaler.transform(arousal)\n",
    "\n",
    "input_melspectrograms = np.reshape(normalized_melspectrograms,(len(normalized_melspectrograms),128, -1,1))\n",
    "print(np.amax(all_spectrograms))\n",
    "print(np.amax(normalized_melspectrograms))\n",
    "\n",
    "print(np.amax(arousal))\n",
    "print(np.amax(normalized_arousal))\n",
    "\n",
    "input_melspectrograms.shape \n",
    "# (400, 128, 427, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(wav_text_data)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "tokenized_text = []\n",
    "max_seq_len = 20\n",
    "\n",
    "for i in range(0, len(wav_text_data)):\n",
    "    new_token_text = tokenizer.texts_to_sequences([wav_text_data[i]])\n",
    "    new_text = pad_sequences(new_token_text, maxlen=max_seq_len)\n",
    "    tokenized_text.append(new_text)\n",
    "\n",
    "input_text = np.array(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_input_file, \"wb\") as f:\n",
    "    pickle.dump(input_melspectrograms, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(pickle_valence_file, \"wb\") as f:\n",
    "    pickle.dump(normalized_valence, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(pickle_arousal_file, \"wb\") as f:\n",
    "    pickle.dump(normalized_arousal, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(pickle_spectrogram_scaler_file, \"wb\") as f:\n",
    "    pickle.dump(scaler, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(pickle_arousal_scaler_file, \"wb\") as f:\n",
    "    pickle.dump(arousal_scaler, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(pickle_valence_scaler_file, \"wb\") as f:\n",
    "    pickle.dump(valence_scaler, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(text_input_file, \"wb\") as f:\n",
    "    pickle.dump(input_text, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(tokenizer_file, \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3427, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_melspectrograms[0]\n",
    "normalized_arousal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech to text and save text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
