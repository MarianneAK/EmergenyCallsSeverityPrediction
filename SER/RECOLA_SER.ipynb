{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from keras.utils.all_utils import plot_model, to_categorical\n",
    "from keras.utils.np_utils  import normalize\n",
    "import random\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.initializers import Constant\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Passing\", category=FutureWarning)\n",
    "\n",
    "seed(555)\n",
    "tf.random.set_seed(555)\n",
    "\n",
    "data_dir = '/home/mabikana/Documents/PhD/SER Code/RECOLA Processing/'\n",
    "input_data = data_dir + 'melspectrograms.pickle'\n",
    "arousal_labels = data_dir + 'arousal.pickle'\n",
    "valence_labels = data_dir + 'valence.pickle'\n",
    "input_text = data_dir + \"text.pickle\"\n",
    "tokenizer = data_dir + \"tokenizer_file.pickle\"\n",
    "\n",
    "sr = 44100\n",
    "dur = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_x, text_data_x, arousal_y, valence_y):\n",
    "\n",
    "    \n",
    "    train_x, test_x, train_text_x, test_text_x, \\\n",
    "        train_arousal_y, test_arousal_y, train_valence_y, test_valence_y = train_test_split(data_x, text_data_x, \n",
    "                                                                                            arousal_y, valence_y,\n",
    "                                                                                            train_size=0.7, shuffle=True)\n",
    "\n",
    "    print(\"Training audio data shape : \", train_x.shape)\n",
    "    print(\"Training text data shape : \", train_text_x.shape)\n",
    "\n",
    "    print(\"Testing audio data shape : \", test_x.shape)\n",
    "    print(\"Testing text data shape : \", test_text_x.shape)\n",
    "\n",
    "    print(\"Arousal Training data shape : \", train_arousal_y.shape)\n",
    "    print(\"Arousal Testing data shape : \", test_arousal_y.shape)\n",
    "    \n",
    "    print(\"Valence Training data shape : \", train_valence_y.shape)\n",
    "    print(\"Valence Testing data shape : \", test_valence_y.shape)\n",
    "    \n",
    "    return train_x, train_text_x, test_x, test_text_x, train_arousal_y, test_arousal_y, train_valence_y, test_valence_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training audio data shape :  (2398, 128, 345, 1)\n",
      "Training text data shape :  (2398, 1, 20)\n",
      "Testing audio data shape :  (1029, 128, 345, 1)\n",
      "Testing text data shape :  (1029, 1, 20)\n",
      "Arousal Training data shape :  (2398, 1)\n",
      "Arousal Testing data shape :  (1029, 1)\n",
      "Valence Training data shape :  (2398, 1)\n",
      "Valence Testing data shape :  (1029, 1)\n"
     ]
    }
   ],
   "source": [
    "data_x = np.load(input_data, allow_pickle=True)\n",
    "data_text_x = np.load(input_text, allow_pickle=True)\n",
    "arousal_y = np.load(arousal_labels, allow_pickle=True)\n",
    "valence_y = np.load(valence_labels, allow_pickle=True)\n",
    "tokenizer = np.load(tokenizer, allow_pickle=True)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_size = 300\n",
    "max_text_len = 20\n",
    "\n",
    "train_x, train_text_x, test_x, test_text_x, train_arousal_y,\\\n",
    "    test_arousal_y, train_valence_y, test_valence_y = split_data(data_x, data_text_x, arousal_y, valence_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(a_list):\n",
    "    half = len(a_list)//2\n",
    "    return a_list[:half], a_list[half:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance correlation coefficient (CCC)-based loss function - using non-inductive statistics\n",
    "def ccc(gold, pred):\n",
    "    gold       = K.squeeze(gold, axis=-1)\n",
    "    pred       = K.squeeze(pred, axis=-1)\n",
    "    gold_mean  = K.mean(gold, axis=-1, keepdims=True)\n",
    "    pred_mean  = K.mean(pred, axis=-1, keepdims=True)\n",
    "    covariance = (gold-gold_mean)*(pred-pred_mean)\n",
    "    gold_var   = K.mean(K.square(gold-gold_mean), axis=-1,  keepdims=True)\n",
    "    pred_var   = K.mean(K.square(pred-pred_mean), axis=-1, keepdims=True)\n",
    "    ccc        = K.constant(2.) * covariance / (gold_var + pred_var + K.square(gold_mean - pred_mean) + K.epsilon())\n",
    "    return ccc\n",
    "\n",
    "\n",
    "def ccc_loss(gold, pred):  \n",
    "    # input (num_batches, seq_len, 1)\n",
    "    ccc_loss   = K.constant(1.) - ccc(gold, pred)\n",
    "\n",
    "    return ccc_loss\n",
    "\n",
    "def get_embedding_matrix(vocab_size, word_index, embedding_dim):\n",
    "    path_to_glove_file = '/home/mabikana/Documents/PhD/SER Code/EmergencyOutcomePrediction/cc.fr.300.vec'\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with open(path_to_glove_file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            splited_line = line.split(\" \")\n",
    "            word = splited_line[0]\n",
    "            coefs = ' '.join(splited_line[1:])\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "    num_tokens = vocab_size \n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    #Converted 6791 words (869 misses)\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "\n",
    "    audio_in = tf.keras.layers.Input(shape=input_shape, dtype='float32')\n",
    "\n",
    "    #LFLB1\n",
    "    model = tf.keras.layers.Conv2D(filters=64,\n",
    "                            kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding='same',\n",
    "                            input_shape=input_shape)(audio_in)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.Activation('elu')(model)\n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)(model)\n",
    "\n",
    "\n",
    "    #LFLB2\n",
    "    model = tf.keras.layers.Conv2D(filters=64,\n",
    "                            kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding='same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.Activation('elu')(model)\n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=4, strides=4)(model)\n",
    "    \n",
    "    #LFLB3\n",
    "    model = tf.keras.layers.Conv2D(filters=128,\n",
    "                            kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding='same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.Activation('elu')(model)\n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=4, strides=4)(model)\n",
    "\n",
    "     #LFLB4\n",
    "    model = tf.keras.layers.Conv2D(filters=128,\n",
    "                            kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding='same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.Activation('elu')(model)\n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=4, strides=4)(model)\n",
    "    model = tf.keras.layers.Reshape((-1, 128))(model)\n",
    "\n",
    "    #LSTM\n",
    "    model = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=False))(model) \n",
    "\n",
    "    \n",
    "    # Output\n",
    "    target_names = ('v', 'a')\n",
    "    model_combined = [tf.keras.layers.Dense(1, name=name)(model) for name in target_names]\n",
    "    \n",
    "    model = tf.keras.Model(audio_in, model_combined) \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n",
    "    \n",
    "    model.compile(loss=ccc_loss, optimizer=opt, metrics=[ccc])\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    plot_model(\n",
    "        model, to_file=data_dir + 'model.png', show_shapes=False, \n",
    "        show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_model(input_shape, max_text_len, embedding_size, vocab_size, word_index):\n",
    "\n",
    "    audio_in = tf.keras.layers.Input(shape=input_shape, dtype='float32')\n",
    "\n",
    "    #LFLB1\n",
    "    model = tf.keras.layers.Conv2D(filters=64,\n",
    "                            kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding='same',\n",
    "                            input_shape=input_shape)(audio_in)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.Activation('elu')(model)\n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)(model)\n",
    "\n",
    "\n",
    "    #LFLB2\n",
    "    model = tf.keras.layers.Conv2D(filters=64,\n",
    "                            kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding='same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.Activation('elu')(model)\n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=4, strides=4)(model)\n",
    "    \n",
    "    #LFLB3\n",
    "    model = tf.keras.layers.Conv2D(filters=128,\n",
    "                            kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding='same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.Activation('elu')(model)\n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=4, strides=4)(model)\n",
    "\n",
    "     #LFLB4\n",
    "    model = tf.keras.layers.Conv2D(filters=128,\n",
    "                            kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding='same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.Activation('elu')(model)\n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=4, strides=4)(model)\n",
    "    model = tf.keras.layers.Reshape((-1, 128))(model)\n",
    "\n",
    "    #LSTM\n",
    "    model = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=False))(model) \n",
    "    \n",
    "\n",
    "    inputs_text = tf.keras.layers.Input(shape=(max_text_len, ), dtype='float32')\n",
    "\n",
    "    net = tf.keras.layers.Embedding(vocab_size,\n",
    "                    embedding_size,\n",
    "                    input_length=max_text_len,\n",
    "                    embeddings_initializer=Constant(get_embedding_matrix(vocab_size, word_index, embedding_size)),\n",
    "                    trainable = False\n",
    "                  )(inputs_text)\n",
    "    net = tf.keras.layers.LSTM(256, return_sequences=True)(net)\n",
    "    net = tf.keras.layers.LSTM(256, return_sequences=True)(net)\n",
    "    net = tf.keras.layers.LSTM(256, return_sequences=False)(net)\n",
    "    net = tf.keras.layers.Dropout(0.3)(net)\n",
    "\n",
    "    final_model = tf.keras.layers.concatenate([model, net])\n",
    "\n",
    "    \n",
    "    # Output\n",
    "    target_names = ('v', 'a')\n",
    "    model_combined = [tf.keras.layers.Dense(1, name=name)(final_model) for name in target_names]\n",
    "    \n",
    "    model = tf.keras.Model([audio_in, inputs_text], model_combined) \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n",
    "    \n",
    "    model.compile(loss=ccc_loss, optimizer=opt, metrics=[ccc])\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    plot_model(\n",
    "        model, to_file=data_dir + 'model.png', show_shapes=False, \n",
    "        show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_attention_model(input_shape, rnn_speech,rnn_text,hidden_lstm_speech,\n",
    "                                    hidden_con,hidden_lstm_text,hidden_dim, max_text_len,\n",
    "                                    vocab_size, embedding_size, word_index):\n",
    "   \n",
    "    ##### Speech BiLSTM\n",
    "    speech_input = tf.keras.layers.Input(shape=input_shape, dtype='float32', name='input_speech')\n",
    "    \n",
    "    #LFLB1\n",
    "    model = tf.keras.layers.Conv2D(filters=64,\n",
    "                            kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding='same',\n",
    "                            input_shape=input_shape)(speech_input)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.Activation('elu')(model)\n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)(model)\n",
    "\n",
    "\n",
    "    #LFLB2\n",
    "    model = tf.keras.layers.Conv2D(filters=64,\n",
    "                            kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding='same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.Activation('elu')(model)\n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=4, strides=4)(model)\n",
    "\n",
    "    #LFLB3\n",
    "    model = tf.keras.layers.Conv2D(filters=128,\n",
    "                            kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding='same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.Activation('elu')(model)\n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=4, strides=4)(model)\n",
    "\n",
    "     #LFLB4\n",
    "    model = tf.keras.layers.Conv2D(filters=128,\n",
    "                            kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding='same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.Activation('elu')(model)\n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=4, strides=4)(model)\n",
    "   \n",
    "     #LFLB5\n",
    "    model = tf.keras.layers.Conv2D(filters=64,\n",
    "                            kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding='same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.Activation('elu')(model)\n",
    "    model = tf.keras.layers.MaxPooling2D(pool_size=1, strides=1)(model)\n",
    "\n",
    "    model = tf.keras.layers.Reshape((-1, 128))(model)\n",
    "\n",
    "    \n",
    "    speech_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_lstm_speech,return_sequences=True))(model)\n",
    "    speech_att   = tf.keras.layers.Dense(hidden_con, activation='tanh')(speech_layer)\n",
    "    \n",
    "    speech_att = tf.keras.layers.Reshape((1, hidden_con))(speech_att)\n",
    "    speech_att_source= np.zeros((len(rnn_speech),hidden_con))\n",
    "    speech_att_input = tf.keras.layers.Input(shape=(hidden_con,),dtype='float32')\n",
    "    speech_att_vec   = tf.keras.layers.Dense(hidden_con, activation='relu')(speech_att_input)\n",
    "    speech_att_vec   = tf.keras.layers.Lambda(lambda x: K.batch_dot(*x, axes=(1,2)))([speech_att_vec,speech_att])\n",
    "    speech_att_vec   = tf.keras.layers.Dense(len(rnn_speech[0]),activation='softmax')(speech_att_vec)\n",
    "    speech_att_vec   = tf.keras.layers.Reshape((len(rnn_speech[0]),1))(speech_att_vec)\n",
    "    speech_output= tf.keras.layers.multiply([speech_att_vec,speech_layer])\n",
    "    speech_output= tf.keras.layers.Lambda(lambda x: K.sum(x, axis=1))(speech_output)\n",
    "    speech_output= tf.keras.layers.Dense(hidden_dim, activation='relu')(speech_output)\n",
    "\n",
    "    ##### Text BiLSTM\n",
    "    ##### The attention source is simply the final hidden layer, not the weight summed sequence\n",
    "    ##### This kind of implementation was done empirically, upon the performance\n",
    "    \n",
    "\n",
    "    text_input = tf.keras.layers.Input(shape=(max_text_len, ), dtype='float32', name='input_text')\n",
    "    net = tf.keras.layers.Embedding(\n",
    "                    input_dim=vocab_size,\n",
    "                    output_dim = embedding_size,\n",
    "                    input_length=max_text_len,\n",
    "                    embeddings_initializer=Constant(get_embedding_matrix(vocab_size, word_index, embedding_size)),\n",
    "                    trainable = False\n",
    "                  )(text_input)\n",
    "\n",
    "    net = tf.keras.layers.LSTM(256, return_sequences=True)(net)\n",
    "    net = tf.keras.layers.LSTM(256, return_sequences=True)(net)\n",
    "    net = tf.keras.layers.LSTM(256, return_sequences=True)(net)\n",
    "    net = tf.keras.layers.Dropout(0.3)(net)\n",
    "\n",
    "    text_fw, text_fw_h, text_fw_c = tf.keras.layers.LSTM(hidden_lstm_text,  return_sequences=True, return_state=True,)(net)\n",
    "    text_bw, text_bw_h, text_bw_c = tf.keras.layers.LSTM(hidden_lstm_text,  return_sequences=True,go_backwards=True, return_state=True,)(net)\n",
    "    text_layer = tf.keras.layers.concatenate([text_fw,text_bw])\n",
    "    text_final = tf.keras.layers.concatenate([text_fw_h,text_bw_h])\n",
    "    text_att = tf.keras.layers.Dense(hidden_con, activation='tanh')(text_layer)\t\n",
    "    text_att_source = np.zeros((len(rnn_text),hidden_con, 1))              # Dummy code\n",
    "    text_att_input  = tf.keras.layers.Input(shape=(hidden_con, 1), dtype='float32')      # Dummy code\t\n",
    "\n",
    "    ##### Exchange phase\n",
    "    speech_att_hop = tf.keras.layers.Dense(hidden_con, activation='relu')(text_final)\t\n",
    "    speech_att_hop = tf.keras.layers.Lambda(lambda x: K.batch_dot(*x, axes=(1,2)))([speech_att_hop,speech_att])\n",
    "    speech_att_hop = tf.keras.layers.Dense(len(rnn_speech[0]),activation='softmax')(speech_att_hop)\n",
    "    speech_att_hop = tf.keras.layers.Reshape((len(rnn_speech[0]),1))(speech_att_hop)\t\n",
    "    speech_output_hop = tf.keras.layers.multiply([speech_att_hop,speech_layer])  # Text-influenced attention for audio\n",
    "    speech_output_hop = tf.keras.layers.Lambda(lambda x: K.sum(x, axis=1))(speech_output_hop)\n",
    "    speech_output_hop = tf.keras.layers.Dense(hidden_dim, activation='relu')(speech_output_hop)\n",
    "    text_att_hop = tf.keras.layers.Dense(hidden_con, activation='relu')(speech_output)\t\n",
    "    text_att_hop = tf.keras.layers.Lambda(lambda x: K.batch_dot(*x, axes=(1,2)))([text_att_hop,text_att])\n",
    "    text_att_hop = tf.keras.layers.Dense(len(rnn_text[0]),activation='softmax')(text_att_hop)\n",
    "    text_att_hop = tf.keras.layers.Reshape((len(rnn_text[0]),1))(text_att_hop)\t\n",
    "    text_output_hop = tf.keras.layers.multiply([text_att_hop,text_layer])        # Audio-influenced attention for text\n",
    "    text_output_hop = tf.keras.layers.Lambda(lambda x: K.sum(x, axis=1))(text_output_hop)\n",
    "    text_output_hop = tf.keras.layers.Dense(hidden_dim, activation='relu')(text_output_hop)\t\n",
    "    \n",
    "    ##### Total output\n",
    "    output    = tf.keras.layers.concatenate([speech_output_hop, text_output_hop])\n",
    "    output    = tf.keras.layers.Dense(hidden_dim, activation='relu')(output)\n",
    "    output    = tf.keras.layers.Dropout(0.3)(output)\n",
    "    output    = tf.keras.layers.Dense(hidden_dim, activation='relu')(output)\n",
    "    output    = tf.keras.layers.Dropout(0.3)(output)\n",
    "    target_names = ('v', 'a')\n",
    "    main_output = [tf.keras.layers.Dense(1, name=name)(output) for name in target_names]\n",
    "    \n",
    "    final_model = tf.keras.Sequential()\n",
    "    final_model = tf.keras.Model([speech_input, speech_att_input, text_input, text_att_input], main_output)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0004, decay=1e-6)\n",
    "\n",
    "    final_model.compile(loss=ccc_loss,optimizer=opt,metrics=[ccc])\n",
    "\n",
    "#     final_model.summary()\n",
    "    plot_model(\n",
    "        final_model, to_file=data_dir + 'cross_attention_model.png', show_shapes=False, \n",
    "        show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",
    "    )\n",
    "\n",
    "    return final_model, speech_att_source, text_att_source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multimodal_model(train_x, train_text_x, validation_x, validation_text_x, train_arousal_y, validation_arousal_y, train_valence_y, validation_valence_y, model):\n",
    "    \n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                mode='min',\n",
    "                verbose=0,\n",
    "                patience=10,\n",
    "                restore_best_weights=True)\n",
    "\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(data_dir + 'model.h5',\n",
    "                            monitor='val_loss',\n",
    "                            mode='min',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True)\n",
    "\n",
    "    model.fit([train_x, train_text_x], [train_valence_y, train_arousal_y],   \n",
    "              validation_data=([validation_x, validation_text_x], [validation_valence_y, validation_arousal_y]),\n",
    "              epochs=100,\n",
    "              batch_size=16,\n",
    "              verbose=2,\n",
    "              callbacks=[es, mc],\n",
    "              shuffle=True)\n",
    "      \n",
    "    return tf.keras.models.load_model(data_dir + 'model.h5', custom_objects={\"ccc_loss\": ccc_loss, \"ccc\": ccc})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_x, validation_x, train_arousal_y, validation_arousal_y, train_valence_y, validation_valence_y, model):\n",
    "    \n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                mode='min',\n",
    "                verbose=0,\n",
    "                patience=10,\n",
    "                restore_best_weights=True)\n",
    "\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(data_dir + 'model.h5',\n",
    "                            monitor='val_loss',\n",
    "                            mode='min',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True)\n",
    "\n",
    "    model.fit([train_x], [train_valence_y, train_arousal_y],   \n",
    "              validation_data=([validation_x], [validation_valence_y, validation_arousal_y]),\n",
    "              epochs=100,\n",
    "              batch_size=16,\n",
    "              verbose=2,\n",
    "              callbacks=[es, mc],\n",
    "              shuffle=True)\n",
    "      \n",
    "    return tf.keras.models.load_model(data_dir + 'model.h5', custom_objects={\"ccc_loss\": ccc_loss, \"ccc\": ccc})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cross_attention_model(train_x, speech_att_source, train_text_x, text_att_source, \n",
    "                                validation_x, speech_att_source_val, validation_text_x, text_att_source_val,\n",
    "                                train_arousal_y, validation_arousal_y, train_valence_y, validation_valence_y, model):\n",
    "    \n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                mode='min',\n",
    "                verbose=0,\n",
    "                patience=10,\n",
    "                restore_best_weights=True)\n",
    "\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(data_dir + 'cross_attention_model_weights.ckpt',\n",
    "                            monitor='val_loss',\n",
    "                            mode='min',\n",
    "                            verbose=1,\n",
    "                            save_weights_only=True,\n",
    "                            save_best_only=True)\n",
    "\n",
    "    original_model = model\n",
    "    \n",
    "    model.fit([train_x, speech_att_source, train_text_x, text_att_source], [train_valence_y, train_arousal_y],   \n",
    "              validation_data=([validation_x, speech_att_source_val, validation_text_x, text_att_source_val], \n",
    "                               [validation_valence_y, validation_arousal_y]),\n",
    "              epochs=100,\n",
    "              batch_size=16,\n",
    "              verbose=2,\n",
    "              callbacks=[es, mc],\n",
    "              shuffle=True)\n",
    "      \n",
    "    return original_model.load_weights(data_dir + 'cross_attention_model_weights.ckpt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multimodal_model(model, test_x, test_train_x, test_valence_y, test_arousal_y):\n",
    "    return model.evaluate([test_x, test_train_x], \n",
    "                         [test_valence_y, test_arousal_y],\n",
    "                         verbose=0)\n",
    "    \n",
    "def evaluate_model(model, test_x, test_valence_y, test_arousal_y):\n",
    "    return model.evaluate([test_x], \n",
    "                         [test_valence_y, test_arousal_y],\n",
    "                         verbose=0)\n",
    "\n",
    "def evaluate_cross_attention_model(model, test_x, speech_att_source_test, test_train_x, text_att_source_test, test_valence_y, test_arousal_y):\n",
    "    return model.evaluate([test_x, speech_att_source_test, test_train_x, text_att_source_test], \n",
    "                         [test_valence_y, test_arousal_y],\n",
    "                         verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on  2398  samples\n",
      "Validating on  514  samples\n",
      "Testing on  514  samples\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.99564, saving model to /home/mabikana/Documents/PhD/SER Code/RECOLA Processing/model.h5\n",
      "150/150 - 14s - loss: 1.0598 - v_loss: 0.7311 - a_loss: 0.3287 - v_ccc: 0.2689 - a_ccc: 0.6713 - val_loss: 1.9956 - val_v_loss: 0.9925 - val_a_loss: 1.0031 - val_v_ccc: 0.0075 - val_a_ccc: -3.0880e-03 - 14s/epoch - 92ms/step\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1.99564\n",
      "150/150 - 6s - loss: 0.9473 - v_loss: 0.6640 - a_loss: 0.2833 - v_ccc: 0.3360 - a_ccc: 0.7167 - val_loss: 2.0119 - val_v_loss: 0.9959 - val_a_loss: 1.0161 - val_v_ccc: 0.0041 - val_a_ccc: -1.6054e-02 - 6s/epoch - 40ms/step\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 1.99564 to 1.86875, saving model to /home/mabikana/Documents/PhD/SER Code/RECOLA Processing/model.h5\n",
      "150/150 - 6s - loss: 0.8393 - v_loss: 0.5687 - a_loss: 0.2707 - v_ccc: 0.4313 - a_ccc: 0.7293 - val_loss: 1.8688 - val_v_loss: 0.8733 - val_a_loss: 0.9954 - val_v_ccc: 0.1267 - val_a_ccc: 0.0046 - 6s/epoch - 41ms/step\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 1.86875 to 1.45581, saving model to /home/mabikana/Documents/PhD/SER Code/RECOLA Processing/model.h5\n",
      "150/150 - 6s - loss: 0.8140 - v_loss: 0.5373 - a_loss: 0.2767 - v_ccc: 0.4627 - a_ccc: 0.7232 - val_loss: 1.4558 - val_v_loss: 0.7889 - val_a_loss: 0.6669 - val_v_ccc: 0.2111 - val_a_ccc: 0.3331 - 6s/epoch - 41ms/step\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 1.45581 to 1.11794, saving model to /home/mabikana/Documents/PhD/SER Code/RECOLA Processing/model.h5\n",
      "150/150 - 6s - loss: 0.7698 - v_loss: 0.5097 - a_loss: 0.2601 - v_ccc: 0.4903 - a_ccc: 0.7399 - val_loss: 1.1179 - val_v_loss: 0.7485 - val_a_loss: 0.3694 - val_v_ccc: 0.2515 - val_a_ccc: 0.6306 - 6s/epoch - 41ms/step\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 1.11794 to 0.94464, saving model to /home/mabikana/Documents/PhD/SER Code/RECOLA Processing/model.h5\n",
      "150/150 - 6s - loss: 0.7040 - v_loss: 0.4436 - a_loss: 0.2604 - v_ccc: 0.5564 - a_ccc: 0.7396 - val_loss: 0.9446 - val_v_loss: 0.6104 - val_a_loss: 0.3342 - val_v_ccc: 0.3896 - val_a_ccc: 0.6658 - 6s/epoch - 42ms/step\n",
      "Epoch 7/100\n"
     ]
    }
   ],
   "source": [
    "validation_x, test_x = split_list(test_x)\n",
    "validation_text_x, test_text_x = split_list(test_text_x)\n",
    "validation_arousal_y, test_arousal_y = split_list(test_arousal_y)\n",
    "validation_valence_y, test_valence_y = split_list(test_valence_y)\n",
    "\n",
    "# new_train_text = train_text_x\n",
    "# new_validation_text = validation_text_x\n",
    "# new_test_text = test_text_x\n",
    "new_train_text = train_text_x.reshape(len(train_text_x), max_text_len)\n",
    "new_validation_text = validation_text_x.reshape(len(validation_text_x), max_text_len)\n",
    "new_test_text = test_text_x.reshape(len(test_text_x), max_text_len)\n",
    "\n",
    "model_architecture = create_model((128, 345, 1))\n",
    "\n",
    "print(\"Training on \", len(train_x), \" samples\")\n",
    "print(\"Validating on \", len(validation_x), \" samples\")\n",
    "print(\"Testing on \", len(validation_x), \" samples\")\n",
    "\n",
    "model = train_model(train_x, validation_x, train_arousal_y, validation_arousal_y, train_valence_y, validation_valence_y, model_architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = evaluate_model(model, test_x, test_valence_y, test_arousal_y)\n",
    "print(\"loss = \", scores[0], \", valence_loss = \", scores[1], \", arousal_loss = \", scores[2], \n",
    "     \", \\n valence_ccc = \", scores[3], \", arousal_ccc = \", scores[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input shape \", train_x.shape)\n",
    "model_cross_attention_architecture, speech_att_source, text_att_source = create_cross_attention_model((128, 345, 1), train_x, new_train_text, 256,128,256,128, max_text_len, vocab_size, embedding_size, word_index)\n",
    "model_cross_attention_architecture_val, speech_att_source_val, text_att_source_val = create_cross_attention_model((128, 345, 1), validation_x, new_validation_text, 256,128,256,128, max_text_len, vocab_size, embedding_size, word_index)\n",
    "\n",
    "print(\"Training on \", len(train_x), \" samples\")\n",
    "print(\"Validating on \", len(validation_x), \" samples\")\n",
    "print(\"Testing on \", len(validation_x), \" samples\")\n",
    "\n",
    "\n",
    "model_cross_attention = train_cross_attention_model(train_x, speech_att_source, new_train_text, text_att_source, \n",
    "                                validation_x, speech_att_source_val, new_validation_text, text_att_source_val,\n",
    "                                train_arousal_y, validation_arousal_y, train_valence_y, validation_valence_y, \n",
    "                                model_cross_attention_architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cross_attention_architecture_test, speech_att_source_test, text_att_source_test = create_cross_attention_model((128, 345, 1), test_x, new_test_text, 256,128,256,128, max_text_len, vocab_size, embedding_size, word_index)\n",
    "model_cross_attention_architecture_test.load_weights(data_dir + 'cross_attention_model_weights.ckpt')\n",
    "\n",
    "scores_cross_attention = evaluate_cross_attention_model(model_cross_attention_architecture_test, test_x, speech_att_source_test, \n",
    "                                                        new_test_text, text_att_source_test, \n",
    "                                                        test_valence_y, test_arousal_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loss = \", scores_cross_attention[0], \", valence_loss = \", scores_cross_attention[1], \", arousal_loss = \", scores_cross_attention[2], \n",
    "     \", \\n valence_ccc = \", scores_cross_attention[3], \", arousal_ccc = \", scores_cross_attention[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
